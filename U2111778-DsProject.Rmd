---
title: "U2111778 Data Science Project"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message = FALSE)
options(repos = c(CRAN = "https://cran.rstudio.com/"))
```

```{r}
# Load data from an RDA file
load("C:\\Users\\Warrick Scaife\\Desktop\\Data Science Project\\yelp_review_small.Rda")
load("C:\\Users\\Warrick Scaife\\Desktop\\Data Science Project\\yelp_user_small.Rda")
```

### Project Overview

This project aims to utilize reviews from the Yelp Dataset to predict stars associated with reviews. The key objectives include:

-   Exploring the review and user data to identify relevant independent variables.
-   Assessing the suitability of different models within the context of the project.
-   Evaluating model performance using metrics such as accuracy and mean-squared error (MSE).

The project's conclusion is drawn based on the findings, and it determines that a random forest approach yields the best performance.

### Project Approach

This project followed an Ad Hoc methodology which focuses on delivering a specific implementation without concern for broader impact or repeatable processes. The main reasons to not follow this methodology include non-scalability, high rework risk and difficulty in teams, however, due to the small one-off nature of this project these downsides were largely nullified. Consequently, the project benefited from a quick start and, drawing inspiration from agile methodologies, it could be iteratively improved without adhering to a predefined methodology.

### Feature Selection

In considering what independent variables to use, domain knowledge provided intuition. For example, variables such as 'cool', 'funny' and 'useful' are presumed to hold importance. We also explore other independent variables constructed from the data, the first of which is 'average_business_stars' which is the average stars given to a business across the reviews in the dataset, this follows the intuition that previous reviews of a given business are likely to provide good intuition on future reviews. On a similar note, we consider the variable 'average_stars' which considers the average stars given by a user across all their reviews, intuitively this will capture whether a given reviewer is naturally cynical or optimistic. Finally, through sentiment analysis, two more variables were created, 'afinn_score' and 'bing_score', these have been constructed by considering the sentiment of each informative word in a review and summing across the review, in the case of Bing, assigning each word either negative one or positive one, or in the case of Afinn, assigning each word values from negative five to positive five. Many variables were not considered within the user data set for two main reasons, significant lack of observations and no direct or attainable link to the stars of reviews.

```{r,message=FALSE,warning=FALSE,include=FALSE}
# Install and load necessary packages
install.packages(c("tidyverse", "tidytext", "gutenbergr", "topicmodels", "textdata", 
                   "rpart", "tree","doParallel","text", "randomForest","corrplot"))

library(tidyverse)   # Data manipulation and visualization
library(tidytext)     # Text analysis with tidy data principles
library(gutenbergr)   # Access Project Gutenberg texts
library(topicmodels)  # Topic modeling
library(dplyr)        # Data manipulation
library(textdata)     # Functions for text data analysis
library(splines)      # B-spline basis functions
library(rpart)        # Recursive Partitioning and Regression Trees
library(ggplot2)      # Data visualization
library(tree)         # Tree-based models
library(caret)        # Classification and Regression Training
library(text)         # Functions for text mining and analysis
library(tm)           # Text mining
library(randomForest) # Random Forest models
library(doParallel)   # Create Hash Digests 
library(corrplot)     # Visualise Correlation
```

```{r}
#Data Preparation
set.seed(1)

# Removing unnecessary columns
review_data_small <- select(review_data_small, -date)

# Calculate average stars per business
average_stars_per_business <- review_data_small %>%
  group_by(business_id) %>%
  summarize(average_business_stars = mean(stars, na.rm = TRUE))

# Add 'average_business_stars' to 'review_data_small' based on 'business_id
review_data_small <- left_join(review_data_small, average_stars_per_business, by = "business_id")

# Add 'average_stars' to 'review_data_small' based on 'user_id'
review_data_small <- review_data_small %>%
  left_join(user_data_small %>% select(user_id, average_stars), by = "user_id")

#Removing any reviews which don't have average_stars
review_data_small <- review_data_small %>%
  filter(!is.na(average_stars))

# Separating data entries by individual words
word_sep_data <- review_data_small %>%
  unnest_tokens(word, text)

# Removing common words
data(stop_words)
word_sep_data <- word_sep_data %>%
  anti_join(stop_words)

# Adding sentiment scores to data
sentiments_afinn <- get_sentiments("afinn")
sentiments_bing <- get_sentiments("bing")

word_sep_data <- word_sep_data %>%
  inner_join(sentiments_afinn) %>%
  inner_join(sentiments_bing) %>%
  mutate(sentiment_numeric = ifelse(sentiment == "negative", -1, 1)) %>%
  select(-sentiment)

# Summing total sentiment for a review and recombining
review_data_final <- word_sep_data %>%
  group_by(review_id, useful, cool, funny, stars, average_stars,average_business_stars) %>%
  summarize(
    afinn_score = sum(value, na.rm = TRUE),
    bing_score = sum(sentiment_numeric, na.rm = TRUE))

# Splitting data into test and training data 

# Randomly sampling indices for test data
test_indices <- sample(1:nrow(review_data_final), 10000)

# Creating training data by excluding test indices
training_data <- review_data_final[-test_indices, ]

# Creating test data by including only test indices
test_data <- review_data_final[test_indices, ]
```

### Exploratory Data Analysis

Understanding the distribution of stars will provide intuition when evaluating models.

```{r}
ggplot(review_data_final, aes(x = factor(stars))) +
  geom_bar(aes(y = ..count../sum(..count..)), fill = "#69b3a2", color = "#404040", width = 0.7) +
  geom_text(stat = "count", aes(label = scales::percent(..count../sum(..count..)),
            y = ..count../sum(..count..)), vjust = -0.5) +
  labs(title = "Distribution of Stars",
       x = "Stars",
       y = "Proportion") +
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(color = "#404040"),
    text = element_text(color = "#404040")
  )
```

47% of reviews are five stars, this may introduce bias into our models through the fact that imbalanced classes might lead a model to be biased towards the majority class, as it seeks to minimise errors. This could result in the minority class being poorly predicted, an aspect later demonstrated.

We also want to consider whether our independent variables are statistically significant in predicting stars. To do this we consider two metrics, correlation and statistical significance, and while these are both linear indicators, they nevertheless contribute insight to the project.

```{r}
#Establishing the relevance of variables
cor_matrix <- cor(review_data_final[, c("cool", "funny", "useful", "average_business_stars", "average_stars", "afinn_score", "bing_score", "stars")])

# Visualize the correlation matrix using a heatmap
corrplot(cor_matrix, method = "color", type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
```

We can note that 'useful', 'cool' and 'funny' have very low correlation with the outcome variable, however are statistically significant. In contrast to all other variables which have moderate positive correlation and are all also statistically significant.

Notably, 'cool', 'useful' and 'funny' are all strongly correlated with each and therefore may have multicollinearity, similarly and intuitively 'bing_score' and 'afinn_score' are highly correlated as they are being generated from the same reviews.

A noteworthy point is the large range in 'afinn_score' and 'bing_score'. Upon investigation, retaining outliers leads to higher accuracy. Additionally, the analysis indicated no relationship between review length and sentiment score.

```{r}
# Create a new variable 'predictions' with constant value 5
base_predictions <- rep(5, nrow(test_data))

# Evaluate accuracy
accuracy_base <- sum(base_predictions == test_data$stars) / length(test_data$stars)

# Linear Model
linear_model <- lm(stars ~ bing_score + afinn_score + useful + funny + cool+average_stars+average_business_stars, data = training_data)

# Make predictions using the linear model
linear_predictions <- predict(linear_model, newdata = test_data)

# Calculate accuracy
accuracy_linear <- sum((round(linear_predictions) == test_data$stars) / length(linear_predictions))

# B-spline Model
spline_model <- lm(stars ~ bs(bing_score) + bs(afinn_score) + bs(useful) + bs(funny) + bs(cool)+bs(average_stars)+bs(average_business_stars), data = training_data)

# Make predictions using the spline model
spline_predictions <- predict(spline_model, newdata = test_data)

# Calculate accuracy
accuracy_spline <- sum((round(spline_predictions) == test_data$stars) / length(spline_predictions))

# Decision Tree Model
training_data$stars <- factor(training_data$stars, levels = c(1, 2, 3, 4, 5))
test_data$stars <- factor(test_data$stars, levels = c(1, 2, 3, 4, 5))

tree_model <- tree(stars ~ bing_score+afinn_score+average_stars+cool+useful+funny+average_business_stars, data = training_data)

# Make predictions using the decision tree model
predictions_tree <- predict(tree_model,newdata = test_data,type = "class")

# Evaluate predictability
accuracy_tree <- sum(predictions_tree == test_data$stars) / length(predictions_tree)

# Decision Tree Model with Pruning
training_data$stars <- factor(training_data$stars, levels = c(1, 2, 3, 4, 5))
test_data$stars <- factor(test_data$stars, levels = c(1, 2, 3, 4, 5))

# Define tree control parameters for pruning
tree_control_params <- tree.control(nobs = nrow(training_data), mincut = 0.1, minsize = 1, mindev = 0.001)

# Create the decision tree model with pruning
tree_model_pruned <- tree(stars ~ bing_score + afinn_score + average_stars + cool + useful + funny + average_business_stars, data = training_data, control = tree_control_params)

# Make predictions using the pruned decision tree model
predictions_tree_pruned <- predict(tree_model_pruned, newdata = test_data, type = "class")

# Evaluate predictability
accuracy_tree_pruned <- sum(predictions_tree_pruned == test_data$stars) / length(predictions_tree_pruned)

# Random Forest Model
cl <- makeCluster(4)  # Adjust the number of cores
registerDoParallel(cl)

randomfor_model <- randomForest(stars ~ afinn_score + bing_score + useful + funny + cool+average_stars+average_business_stars, data = training_data, ntree = 50)

# Make predictions using the random forest model
rf_predictions <- predict(randomfor_model, newdata = test_data)

# Calculate accuracy
accuracy_rf <- sum(rf_predictions == test_data$stars) / length(rf_predictions)

# Close parallel processing cluster
stopCluster(cl)
```

```{r}
#Calculating the MSE for models:

# Convert 'stars' to numeric in the test_data
test_data$stars <- as.numeric(as.character(test_data$stars))

# Base Model
mse_base <- mean((test_data$stars - base_predictions)^2)

# Linear Model
mse_linear <- mean((test_data$stars - linear_predictions)^2)

# B-spline Model
mse_spline <- mean((test_data$stars - spline_predictions)^2)

# Decision Tree Model
mse_tree <- mean((test_data$stars - as.numeric(predictions_tree))^2)

# Pruned Decision Tree Model
mse_pruned <- mean((test_data$stars - as.numeric(predictions_tree_pruned))^2)

# Random Forest Model
mse_rf <- mean((test_data$stars - as.numeric(rf_predictions))^2)
```

### Model Selection and Analysis

This section will analyse the exploration of different models, how they performed, identifying limitations and how barriers encountered were overcome. Before discussing model selection we must establish evaluation criteria. First, a straightforward measure called accuracy represents the proportion of correct predictions. Secondly, a more statistical measure MSE. An effective model will have high accuracy, indicating many correct predictions, and low MSE, signifying incorrect predictions are close to actual values. We include a benchmark model which only predicts five stars.

#### Classification Trees

Considering the categorical nature of stars and the predictive nature of the project, an intuitive area to commence analysis is classification trees.

```{r}
# Plot the decision tree
plot(tree_model, col = "black", lty = 1, lwd = 3, cex = 1.2, gap = 10)
text(tree_model, pretty = 0, cex = 1, col="gray",font=2,bg="black",srt=0)
```

While performance in both metrics has improved compared to the benchmark, MSE is still high. Additionally, branches having identical outcomes is a sign of the model overfitting the data, meaning that the model has captured noise or patterns from the training data which do not generalise to the test data. This may suggest that the model is too complex and requires pruning.

Limiting the depth through pruning will help the issue of overfitting by removing branches that do not contribute significantly to improving the model's performance.

```{r}
# Plot the pruned decision tree
plot(tree_model_pruned, col = "black", lty = 1, lwd = 3, cex = 1.2, gap = 10)
text(tree_model_pruned, pretty = 0, cex = 1, col="gray",font=2,bg="black",srt=0)
```

Demonstrated is the result of pruning at a low-cost complexity, which, while it does improve the performance of the model, the issue of overfitting has not been solved, and the model has lost its interpretability. Increasing the cost complexity of the model has little effect, merely reducing the performance and returning to the previous model.

Both of these cases have illustrated the fact that classification trees are not robust, imbalances in the data have resulted in a big effect on the final tree and ultimately MSE.

#### Linear Models

To overcome this issue we can consider a more restrictive linear model which is more robust to deviations in the data.

While this model does reduce the MSE, it suffers from a lower predictability. One way to stay in a linear realm whilst improving predictability is to consider a spline model to introduce some flexibility and maintain low MSE.

This model does further reduce MSE, however does not massively improve the accuracy. We now seek a model which maintains the low MSE of linear models whilst keeping high predictability.

#### Random Forest

To increase predictability whilst maintaining a low MSE we return to a classification model, however, to reduce the bias introduced within the model, we employ a random forest methodology which reduces variance, bias and ultimately MSE.

From this analysis it is evident that random forest demonstrated superior predictability with MSE. This is attributed to the inherent characteristics of random forests, enabling them to overcome the challenges posed by the imbalanced dataset and multicollinearity.

#### Model Results

| Model         | Accuracy | MSE  |
|:--------------|:--------:|:----:|
| Benchmark     |   47%    | 3.7  |
| Tree          |   56%    | 2.12 |
| Pruned Tree   |   59%    | 1.38 |
| Linear        |   41%    | 0.94 |
| Spline        |   43%    | 0.89 |
| Random Forest |   61%    | 1.2  |

### Conclusion

We have seen how the different models have a trade-off between MSE and accuracy through overfitting or underfitting the data. The distribution of the data posed significant challenges to classification models and the categorical nature of stars posed challenges for continuous linear models. The optimal model was predictive, achieving high accuracy, and effectively managed overfitting through bootstrapping samples. Additionally, it's ability to handle imbalanced data contributed to its superior performance.

Further analysis into patterns or trends in specifications may indicate where model optimisations can be made. Additionally, increasing the size of validation data will provide more robust results, in combination with other validation methods.

### Challenges

The main challenge encountered was model optimisation, be it independent variables choice or model specification, a comprehensive understanding of data and model is essential for enhancing model performance. Improvement is iterative, and while the models did not perform perfectly, significant enhancements were made through understanding shortcomings and exploring various solutions. This was notably evident in the evolution of the classification tree.
